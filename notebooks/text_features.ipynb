{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bc368a8-20ff-431a-8c8e-500d711332d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27746aa9-1594-47c8-a61a-438ed8ca7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../raw_data/model_v1_labeled_data.csv',index_col=[0])\n",
    "df_copy=df.copy()\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3669f9ca-df98-4a77-b374-255a52179d1b",
   "metadata": {},
   "source": [
    "# Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c629ca-deb0-44b3-8d34-afb097077ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "import unidecode\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleaning(sentence, move_punc=True):\n",
    "    \n",
    "    # Basic cleaning\n",
    "    sentence = sentence.replace('\\n',' ')\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercasing \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## removing numbers\n",
    "    sentence = unidecode.unidecode(sentence) # remove accents\n",
    "    if move_punc==True:\n",
    "        for punctuation in string.punctuation:\n",
    "            sentence = sentence.replace(punctuation, '') ## removing punctuation\n",
    "    # Advanced cleaning        \n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenizing \n",
    "    stop_words = set(stopwords.words('english')) ## defining stopwords\n",
    "    tokenized_sentence = [w for w in tokenized_sentence \n",
    "                                  if not w in stop_words] ## remove stopwords\n",
    "    lemmatized_sentence = [WordNetLemmatizer().lemmatize(word, get_wordnet_pos(word))  # v --> verbs\n",
    "              for word in tokenized_sentence]\n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized_sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "\n",
    "#functions to print top words of each topic\n",
    "def topic_word(vectorizer, model, topic, topwords, with_weights = True):\n",
    "    \"\"\"returns the top words with their weights for one topic\"\"\"\n",
    "    topwords_indexes = topic.argsort()[:-topwords - 1:-1]\n",
    "    if with_weights == True:\n",
    "        topwords = [(vectorizer.get_feature_names_out()[i], round(topic[i],2)) for i in topwords_indexes]\n",
    "    if with_weights == False:\n",
    "        topwords = [vectorizer.get_feature_names_out()[i] for i in topwords_indexes]\n",
    "    return topwords\n",
    "\n",
    "\n",
    "def print_topics(vectorizer, model, topwords):\n",
    "    \"\"\"prints the different topics found by the LDA with their topwords\"\"\"\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"-\"*20)\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print(topic_word(vectorizer, model, topic, topwords))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c66ad-ad28-41b7-a8fe-a6838e1e4a52",
   "metadata": {},
   "source": [
    "# Investigate most frequent words in their occupation(specialities/focus/populations) and goals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897243c5-6ed0-4386-b3f9-269ea0f3318d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    0.26\n",
       "0    0.25\n",
       "1    0.22\n",
       "2    0.14\n",
       "5    0.09\n",
       "3    0.03\n",
       "Name: cluster_id, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a column for their professional profile\n",
    "df=df_copy\n",
    "df['profile']=df.specialities+' '+df.focus+ ' '+df.population+ ' '+df.typeOfPractice\n",
    "df=df[['userID','cluster_id','profile','metaGoalTitle']]\n",
    "\n",
    "\n",
    "# clean the txt columns\n",
    "df=df.dropna()\n",
    "\n",
    "\n",
    "# create a column for cleaned data\n",
    "df['goalCleaned']=df['metaGoalTitle'].apply(cleaning)\n",
    "df['profileCleaned']=df['profile'].apply(cleaning)\n",
    "# distribution of users have goals\n",
    "round(df[\"cluster_id\"].value_counts(normalize = True),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93d72dc9-3269-40b4-acd6-ead87a817ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "732614a0-1898-4714-b11f-cbcf41bf9f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vectorized_goals.shape = (6089, 3000)\n",
      "\n",
      "---------------Key words in the goals of Cluster0 are \n",
      "cpd year              38.166161\n",
      "end cpd               37.832512\n",
      "end cpd year          36.395117\n",
      "would like            33.532472\n",
      "update knowledge      32.941311\n",
      "year want             27.826467\n",
      "wound care            24.235236\n",
      "mental health         23.475285\n",
      "increase knowledge    22.339344\n",
      "cpd year want         21.703222\n",
      "Name: 0.0, dtype: float64\n",
      "\n",
      "---------------Key words in the goals of Cluster1 are \n",
      "end cpd             33.922300\n",
      "cpd year            33.622147\n",
      "update knowledge    32.030518\n",
      "end cpd year        31.935666\n",
      "would like          28.372427\n",
      "wound care          25.517512\n",
      "year want           23.382694\n",
      "age care            18.484195\n",
      "well understand     17.927555\n",
      "cpd year want       17.884961\n",
      "Name: 1.0, dtype: float64\n",
      "\n",
      "---------------Key words in the goals of Cluster2 are \n",
      "would like           22.020536\n",
      "cpd year             19.625711\n",
      "end cpd              18.690704\n",
      "end cpd year         18.317700\n",
      "update knowledge     17.487460\n",
      "wound care           13.957676\n",
      "year want            12.848854\n",
      "well understand      12.526008\n",
      "improve knowledge    11.834750\n",
      "end year             11.265622\n",
      "Name: 2.0, dtype: float64\n",
      "\n",
      "---------------Key words in the goals of Cluster3 are \n",
      "update knowledge      5.997433\n",
      "would like            5.315676\n",
      "increase knowledge    4.810716\n",
      "cpd year              4.453519\n",
      "end cpd year          4.405008\n",
      "end cpd               4.403515\n",
      "year want             3.964634\n",
      "cpd year want         3.142119\n",
      "improve knowledge     2.994200\n",
      "wound care            2.989694\n",
      "Name: 3.0, dtype: float64\n",
      "\n",
      "---------------Key words in the goals of Cluster4 are \n",
      "update knowledge      37.126526\n",
      "end cpd               35.488688\n",
      "would like            35.249627\n",
      "cpd year              33.451717\n",
      "end cpd year          31.679006\n",
      "year want             25.749366\n",
      "mental health         23.626326\n",
      "increase knowledge    22.543055\n",
      "wound care            18.987500\n",
      "cpd year want         18.811774\n",
      "Name: 4.0, dtype: float64\n",
      "\n",
      "---------------Key words in the goals of Cluster5 are \n",
      "end cpd               16.400358\n",
      "cpd year              16.204083\n",
      "end cpd year          15.576022\n",
      "would like            13.991376\n",
      "year want             11.991572\n",
      "update knowledge      10.782236\n",
      "wound care             9.755258\n",
      "cpd year want          9.649723\n",
      "increase knowledge     8.710620\n",
      "end year               8.487394\n",
      "Name: 5.0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Vectorizers and NLP Models\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.75,max_features=3000,ngram_range=(2,3))\n",
    "vectorized_goals = pd.DataFrame(vectorizer.fit_transform(df[\"goalCleaned\"]).toarray(),\n",
    "                                 columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(f\" vectorized_goals.shape = {vectorized_goals.shape}\")\n",
    "vectorized_goals['cluster_id']=df['cluster_id']\n",
    "grp_vec_goals=vectorized_goals.groupby('cluster_id').sum().T\n",
    "grp_vec_goals\n",
    "\n",
    "for i in range(6):\n",
    "    print(f'\\n---------------Key words in the goals of Cluster{i} are ')\n",
    "    \n",
    "    df0=grp_vec_goals.iloc[:,i].sort_values(ascending=False)\n",
    "    print(df0.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bf799-b5b7-46f9-beb8-3cfa5c08aa0b",
   "metadata": {},
   "source": [
    "## Investigate profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55c578af-2c87-4afa-a6bb-c490b98e5c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vectorized_goals.shape = (6089, 3001)\n",
      "\n",
      "---------------Key words in the specialities/focus/population of Cluster0 are \n",
      "practice                    51.121715\n",
      "clinical                    47.372365\n",
      "clinical practice           46.962448\n",
      "care                        46.008897\n",
      "adults                      44.689800\n",
      "hospital                    44.306996\n",
      "clinical practice adults    42.414390\n",
      "practice adults             42.405105\n",
      "adults hospital             41.164339\n",
      "practice adults hospital    38.941693\n",
      "Name: 0.0, dtype: float64\n",
      "\n",
      "---------------Key words in the specialities/focus/population of Cluster1 are \n",
      "practice                             46.779280\n",
      "care                                 45.599062\n",
      "clinical                             43.162727\n",
      "clinical practice                    42.807662\n",
      "hospital                             40.201527\n",
      "adults                               39.686212\n",
      "clinical practice adults             38.694098\n",
      "practice adults                      38.685627\n",
      "adults hospital                      37.861224\n",
      "clinical practice adults hospital    36.253580\n",
      "Name: 1.0, dtype: float64\n",
      "\n",
      "---------------Key words in the specialities/focus/population of Cluster2 are \n",
      "practice                    30.272329\n",
      "clinical                    28.334356\n",
      "clinical practice           27.957072\n",
      "care                        27.328312\n",
      "adults                      25.754425\n",
      "hospital                    25.740318\n",
      "practice adults             25.210861\n",
      "clinical practice adults    25.172638\n",
      "adults hospital             23.357455\n",
      "practice adults hospital    22.847757\n",
      "Name: 2.0, dtype: float64\n",
      "\n",
      "---------------Key words in the specialities/focus/population of Cluster3 are \n",
      "care                        7.720275\n",
      "practice                    6.998801\n",
      "aged                        6.392333\n",
      "clinical                    6.369199\n",
      "aged care                   6.364525\n",
      "clinical practice           6.259810\n",
      "hospital                    6.050282\n",
      "adults                      5.571836\n",
      "clinical practice adults    5.312971\n",
      "practice adults             5.311808\n",
      "Name: 3.0, dtype: float64\n",
      "\n",
      "---------------Key words in the specialities/focus/population of Cluster4 are \n",
      "practice                             52.433794\n",
      "clinical                             49.157619\n",
      "care                                 48.703857\n",
      "clinical practice                    48.269252\n",
      "adults                               45.576178\n",
      "hospital                             44.951401\n",
      "clinical practice adults             44.213691\n",
      "practice adults                      44.204011\n",
      "adults hospital                      40.884124\n",
      "clinical practice adults hospital    38.987998\n",
      "Name: 4.0, dtype: float64\n",
      "\n",
      "---------------Key words in the specialities/focus/population of Cluster5 are \n",
      "practice                    17.421083\n",
      "care                        16.990949\n",
      "clinical                    15.668692\n",
      "clinical practice           15.578481\n",
      "hospital                    15.031326\n",
      "adults                      14.405388\n",
      "adults hospital             13.121320\n",
      "aged                        12.963415\n",
      "aged care                   12.953628\n",
      "clinical practice adults    12.579772\n",
      "Name: 5.0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.75,ngram_range=(1,5))\n",
    "vectorized_profs = pd.DataFrame(vectorizer.fit_transform(df[\"profile\"]).toarray(),\n",
    "                                 columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(f\" vectorized_goals.shape = {vectorized_goals.shape}\")\n",
    "vectorized_profs['cluster_id']=df['cluster_id']\n",
    "grp_vec_profs=vectorized_profs.groupby('cluster_id').sum().T\n",
    "grp_vec_profs\n",
    "\n",
    "for i in range(6):\n",
    "    print(f'\\n---------------Key words in the specialities/focus/population of Cluster{i} are ')\n",
    "    \n",
    "    df0=grp_vec_profs.iloc[:,i].sort_values(ascending=False)\n",
    "    print(df0.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7e167-5177-4a38-994d-360fc6253698",
   "metadata": {},
   "source": [
    "# LDA on all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f6d2367-b01e-4e14-9623-df0c35b1f928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    0.26\n",
       "0    0.25\n",
       "1    0.22\n",
       "2    0.14\n",
       "5    0.09\n",
       "3    0.03\n",
       "Name: cluster_id, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "\n",
    "\n",
    "# create a column for cleaned data\n",
    "df['goalCleaned']=df['metaGoalTitle'].apply(cleaning)\n",
    "\n",
    "# distribution of users have goals\n",
    "round(df[\"cluster_id\"].value_counts(normalize = True),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "999c3dce-d4ef-40a3-9baf-e66569987d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vectorized_goals.shape = (6089, 251349)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa perform</th>\n",
       "      <th>aaa perform suitable</th>\n",
       "      <th>aaa repair</th>\n",
       "      <th>aaa repair surgery</th>\n",
       "      <th>aag pca</th>\n",
       "      <th>aag pca nswnma</th>\n",
       "      <th>aastn annual</th>\n",
       "      <th>aastn annual conference</th>\n",
       "      <th>aastn meet</th>\n",
       "      <th>aastn meet company</th>\n",
       "      <th>...</th>\n",
       "      <th>zone prevention</th>\n",
       "      <th>zone prevention improve</th>\n",
       "      <th>zoom session</th>\n",
       "      <th>zoom session effectively</th>\n",
       "      <th>zostavax need</th>\n",
       "      <th>zostavax need update</th>\n",
       "      <th>zoster elderly</th>\n",
       "      <th>zoster elderly able</th>\n",
       "      <th>zwarteveen kim</th>\n",
       "      <th>zwarteveen kim nguyen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 251349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa perform  aaa perform suitable  aaa repair  aaa repair surgery  aag pca  \\\n",
       "0          0.0                   0.0         0.0                 0.0      0.0   \n",
       "1          0.0                   0.0         0.0                 0.0      0.0   \n",
       "2          0.0                   0.0         0.0                 0.0      0.0   \n",
       "3          0.0                   0.0         0.0                 0.0      0.0   \n",
       "4          0.0                   0.0         0.0                 0.0      0.0   \n",
       "\n",
       "   aag pca nswnma  aastn annual  aastn annual conference  aastn meet  \\\n",
       "0             0.0           0.0                      0.0         0.0   \n",
       "1             0.0           0.0                      0.0         0.0   \n",
       "2             0.0           0.0                      0.0         0.0   \n",
       "3             0.0           0.0                      0.0         0.0   \n",
       "4             0.0           0.0                      0.0         0.0   \n",
       "\n",
       "   aastn meet company  ...  zone prevention  zone prevention improve  \\\n",
       "0                 0.0  ...              0.0                      0.0   \n",
       "1                 0.0  ...              0.0                      0.0   \n",
       "2                 0.0  ...              0.0                      0.0   \n",
       "3                 0.0  ...              0.0                      0.0   \n",
       "4                 0.0  ...              0.0                      0.0   \n",
       "\n",
       "   zoom session  zoom session effectively  zostavax need  \\\n",
       "0           0.0                       0.0            0.0   \n",
       "1           0.0                       0.0            0.0   \n",
       "2           0.0                       0.0            0.0   \n",
       "3           0.0                       0.0            0.0   \n",
       "4           0.0                       0.0            0.0   \n",
       "\n",
       "   zostavax need update  zoster elderly  zoster elderly able  zwarteveen kim  \\\n",
       "0                   0.0             0.0                  0.0             0.0   \n",
       "1                   0.0             0.0                  0.0             0.0   \n",
       "2                   0.0             0.0                  0.0             0.0   \n",
       "3                   0.0             0.0                  0.0             0.0   \n",
       "4                   0.0             0.0                  0.0             0.0   \n",
       "\n",
       "   zwarteveen kim nguyen  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "\n",
       "[5 rows x 251349 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizers and NLP Models\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# lets first look at cluster 0\n",
    "#df0=df[df.cluster_id==0]\n",
    "df0=df.copy()\n",
    "vectorizer = TfidfVectorizer(max_df=0.75,ngram_range=(2,3))\n",
    "vectorized_goals = pd.DataFrame(vectorizer.fit_transform(df0[\"goalCleaned\"]).toarray(),\n",
    "                                 columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(f\" vectorized_goals.shape = {vectorized_goals.shape}\")\n",
    "vectorized_goals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be0a11ff-0dc2-4f9b-94a6-4a7ca88dc069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit an LDA\n",
    "n_components = 3\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = n_components)\n",
    "lda.fit(vectorized_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d95bd8ef-45b1-479d-a46a-1d34db7b1599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the  Document Mixture (of Topic) (6089, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6084</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6089 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic 1  Topic 2  Topic 3\n",
       "0        0.03     0.03     0.94\n",
       "1        0.06     0.06     0.88\n",
       "2        0.90     0.05     0.05\n",
       "3        0.90     0.05     0.05\n",
       "4        0.08     0.85     0.07\n",
       "...       ...      ...      ...\n",
       "6084     0.07     0.87     0.07\n",
       "6085     0.84     0.08     0.08\n",
       "6086     0.92     0.04     0.04\n",
       "6087     0.94     0.03     0.03\n",
       "6088     0.04     0.04     0.92\n",
       "\n",
       "[6089 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Mixture (of Topic)\n",
    "document_mixture = lda.transform(vectorized_goals)\n",
    "print(f'the shape of the  Document Mixture (of Topic) {document_mixture.shape}')\n",
    "round(pd.DataFrame(document_mixture, \n",
    "                   columns = [f\"Topic {i+1}\" for i in range(n_components)])\n",
    "      ,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ca8f0bb-aed8-4e49-8b64-79ad1009acc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>profile</th>\n",
       "      <th>metaGoalTitle</th>\n",
       "      <th>goalCleaned</th>\n",
       "      <th>profileCleaned</th>\n",
       "      <th>most_important_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>001939e3-1906-4c78-a2e3-b1a3b8d99a0d</td>\n",
       "      <td>0</td>\n",
       "      <td>Aged Care clinical practice older people resid...</td>\n",
       "      <td>I need to know more about my field of work as ...</td>\n",
       "      <td>need know field work rn age care always evolve...</td>\n",
       "      <td>age care clinical practice old people resident...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0029e22b-80c4-4568-87d7-885b065221e0</td>\n",
       "      <td>0</td>\n",
       "      <td>Orthopaedics,Acute pain,Chronic pain,Preoperat...</td>\n",
       "      <td>Have a better understanding of drug addiction ...</td>\n",
       "      <td>well understand drug addiction understand best...</td>\n",
       "      <td>orthopaedicsacute painchronic painpreoperative...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0096dc53-ce9d-42ab-838e-a9983fd0d172</td>\n",
       "      <td>0</td>\n",
       "      <td>Residential Aged Care clinical practice older ...</td>\n",
       "      <td>Learning about wounds and how to care for them...</td>\n",
       "      <td>learn wound care community age care get well t...</td>\n",
       "      <td>residential age care clinical practice old peo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0196e935-821b-49cc-a899-34c409047f21</td>\n",
       "      <td>0</td>\n",
       "      <td>Orthopaedics,Anaesthetics,Recovery clinical pr...</td>\n",
       "      <td>To become competent in Anaesthetic nursing I w...</td>\n",
       "      <td>become competent anaesthetic nursing would ide...</td>\n",
       "      <td>orthopaedicsanaestheticsrecovery clinical prac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>01d17791-4a67-43c9-b35f-06404ba4108e</td>\n",
       "      <td>1</td>\n",
       "      <td>Acute care clinical practice adults hospital</td>\n",
       "      <td>By the end of CPD year i want to improve gener...</td>\n",
       "      <td>end cpd year want improve general knowledge cl...</td>\n",
       "      <td>acute care clinical practice adult hospital</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   userID  cluster_id  \\\n",
       "115  001939e3-1906-4c78-a2e3-b1a3b8d99a0d           0   \n",
       "116  0029e22b-80c4-4568-87d7-885b065221e0           0   \n",
       "117  0096dc53-ce9d-42ab-838e-a9983fd0d172           0   \n",
       "118  0196e935-821b-49cc-a899-34c409047f21           0   \n",
       "120  01d17791-4a67-43c9-b35f-06404ba4108e           1   \n",
       "\n",
       "                                               profile  \\\n",
       "115  Aged Care clinical practice older people resid...   \n",
       "116  Orthopaedics,Acute pain,Chronic pain,Preoperat...   \n",
       "117  Residential Aged Care clinical practice older ...   \n",
       "118  Orthopaedics,Anaesthetics,Recovery clinical pr...   \n",
       "120       Acute care clinical practice adults hospital   \n",
       "\n",
       "                                         metaGoalTitle  \\\n",
       "115  I need to know more about my field of work as ...   \n",
       "116  Have a better understanding of drug addiction ...   \n",
       "117  Learning about wounds and how to care for them...   \n",
       "118  To become competent in Anaesthetic nursing I w...   \n",
       "120  By the end of CPD year i want to improve gener...   \n",
       "\n",
       "                                           goalCleaned  \\\n",
       "115  need know field work rn age care always evolve...   \n",
       "116  well understand drug addiction understand best...   \n",
       "117  learn wound care community age care get well t...   \n",
       "118  become competent anaesthetic nursing would ide...   \n",
       "120  end cpd year want improve general knowledge cl...   \n",
       "\n",
       "                                        profileCleaned  most_important_topic  \n",
       "115  age care clinical practice old people resident...                     2  \n",
       "116  orthopaedicsacute painchronic painpreoperative...                     2  \n",
       "117  residential age care clinical practice old peo...                     0  \n",
       "118  orthopaedicsanaestheticsrecovery clinical prac...                     0  \n",
       "120        acute care clinical practice adult hospital                     1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#report the most important topic for each review\n",
    "import numpy as np\n",
    "df0[\"most_important_topic\"] = np.argmax(document_mixture, axis = 1)\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dd77887-9be4-4e50-81de-da0bf08d9b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 251349)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Topic Mixture (of Words)\n",
    "topic_mixture = pd.DataFrame(lda.components_, \n",
    "                             columns = vectorizer.get_feature_names_out())\n",
    "topic_mixture.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe54506e-514c-4862-bfae-f9c35c2fa82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Topic 0:\n",
      "[('update knowledge', 42.05), ('would like', 29.96), ('cpd year', 27.84), ('end cpd', 27.15), ('end cpd year', 26.41), ('knowledge wound', 26.31), ('update knowledge wound', 23.88), ('wound dressing', 22.63), ('knowledge wound dressing', 19.91), ('increase knowledge', 19.71)]\n",
      "--------------------\n",
      "Topic 1:\n",
      "[('end cpd', 28.0), ('cpd year', 27.87), ('end cpd year', 26.96), ('wound care', 24.82), ('year want', 22.51), ('would like', 19.92), ('update knowledge', 18.89), ('cpd year want', 18.27), ('mental health', 15.22), ('end year', 13.48)]\n",
      "--------------------\n",
      "Topic 2:\n",
      "[('would like', 20.2), ('end cpd', 19.09), ('cpd year', 17.68), ('end cpd year', 16.94), ('year want', 16.45), ('update knowledge', 16.01), ('age care', 12.75), ('end year', 12.12), ('cpd year want', 11.51), ('improve knowledge', 9.92)]\n"
     ]
    }
   ],
   "source": [
    "#functions to print top words of each topic\n",
    "def topic_word(vectorizer, model, topic, topwords, with_weights = True):\n",
    "    \"\"\"returns the top words with their weights for one topic\"\"\"\n",
    "    topwords_indexes = topic.argsort()[:-topwords - 1:-1]\n",
    "    if with_weights == True:\n",
    "        topwords = [(vectorizer.get_feature_names_out()[i], round(topic[i],2)) for i in topwords_indexes]\n",
    "    if with_weights == False:\n",
    "        topwords = [vectorizer.get_feature_names_out()[i] for i in topwords_indexes]\n",
    "    return topwords\n",
    "\n",
    "\n",
    "def print_topics(vectorizer, model, topwords):\n",
    "    \"\"\"prints the different topics found by the LDA with their topwords\"\"\"\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"-\"*20)\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print(topic_word(vectorizer, model, topic, topwords))\n",
    "        \n",
    "print_topics(vectorizer, lda, topwords = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bc3e68f-2a9e-40ed-a89a-69d783d4bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract important words for each user\n",
    "topic_word_mixture = [topic_word(vectorizer, lda, topic, topwords = 5, with_weights = False)\n",
    "                      for topic in lda.components_]\n",
    "topic_word_mixture\n",
    "df0[\"most_important_words\"] = df0[\"most_important_topic\"].apply(lambda i: topic_word_mixture[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f0548-a421-4d6b-8450-c9bcfcb38a8e",
   "metadata": {},
   "source": [
    "## Make it a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb921cdf-792e-4190-b68a-85b003d31fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vectorized_goals.shape = (6089, 251349)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cluster_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     df0[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost_important_words\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df0[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost_important_topic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m i: topic_word_mixture[i])\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df0\n\u001b[0;32m---> 54\u001b[0m df1\u001b[38;5;241m=\u001b[39m\u001b[43mlda_key_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcol_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoalCleaned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_topic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mlda_key_words\u001b[0;34m(df, col_name, n_topic, n_words)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#Topic Mixture (of Words)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m topic_mixture \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(lda\u001b[38;5;241m.\u001b[39mcomponents_, columns \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Users in Cluster\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cares about\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m print_topics(vectorizer, lda, topwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# extract important words for each user\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_id' is not defined"
     ]
    }
   ],
   "source": [
    "## Investigting each clusters\n",
    "import pandas as pd\n",
    "df=pd.read_csv('../raw_data/model_v3_labeled_data.csv',index_col=[0])\n",
    "\n",
    "# clean the txt columns\n",
    "df=df.dropna()\n",
    "df['profile']=df.specialities+' '+df.focus+ ' '+df.population+ ' '+df.typeOfPractice\n",
    "df=df[['userID','cluster_id','profile','metaGoalTitle']]\n",
    "\n",
    "# create a column for cleaned data\n",
    "df['goalCleaned']=df['metaGoalTitle'].apply(cleaning)\n",
    "df['profileCleaned']=df['profile'].apply(cleaning)\n",
    "\n",
    "def lda_key_words(df,col_name=\"goalCleaned\",n_topic=3, n_words=10):\n",
    "    \n",
    "    df0=df\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df=2/60249,max_df=0.75,ngram_range=(2,3))\n",
    "    vectorized_txt = pd.DataFrame(vectorizer.fit_transform(df0[col_name]).toarray(),\n",
    "                                     columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "    print(f\" vectorized_goals.shape = {vectorized_goals.shape}\")\n",
    "    vectorized_txt.head()\n",
    "    \n",
    "    n_components = n_topic\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components = n_components)\n",
    "    lda.fit(vectorized_txt)\n",
    "    \n",
    "    \n",
    "    #report the most important topic for each review\n",
    "    document_mixture = lda.transform(vectorized_txt)\n",
    "    df0[\"most_important_topic\"] = np.argmax(document_mixture, axis = 1)\n",
    "    #Topic Mixture (of Words)\n",
    "    topic_mixture = pd.DataFrame(lda.components_, columns = vectorizer.get_feature_names_out())\n",
    "    \n",
    "    print(f'\\n Users in Cluster{cluster_id} cares about')\n",
    "    print_topics(vectorizer, lda, topwords = 10)\n",
    "    \n",
    "    \n",
    "    # extract important words for each user\n",
    "    topic_word_mixture = [topic_word(vectorizer, lda, topic, topwords = 5, with_weights = False) \n",
    "                          for topic in lda.components_]\n",
    "    \n",
    "    df0[\"most_important_words\"] = df0[\"most_important_topic\"].apply(lambda i: topic_word_mixture[i])\n",
    "    \n",
    "    return df0\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "df1=lda_key_words(df,col_name=\"goalCleaned\",n_topic=8, n_words=10)\n",
    "#df.head()   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# goals_dic={}\n",
    "# for i in range(n_clusters):\n",
    "#     print(f\"\\n------------Cluster{i}----------------\")\n",
    "#     goals=df0[df0.cluster_id==i][[\"most_important_topic\",\"most_important_words\"]]\n",
    "    \n",
    "#     print('the distribution of topics is')\n",
    "#     print(round(goals[\"most_important_topic\"].value_counts(normalize=True),2))\n",
    "    \n",
    "#     frequency = list(goals[\"most_important_topic\"].value_counts().index)\n",
    "    \n",
    "#     print('\\n The most frequent words are ')\n",
    "#     print([topic_word_mixture[i] for i in frequency])\n",
    "    \n",
    "#     # # creat a DataFrame to save this \n",
    "#     # dfi=pd.DataFrame({\n",
    "#     # 'Topics': \n",
    "    \n",
    "#     #})\n",
    "#     #goals_dic[f'Cluster{i}']=dfi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11994a1e-48ec-4ccd-a11c-418512aa5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52930e-56ae-48a3-b2a9-b02ff1f96e80",
   "metadata": {},
   "source": [
    "# full pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf043a-5db5-4f10-b99d-2b0c4e08c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "max_df = 0.75\n",
    "max_features = 5000\n",
    "ngram_range = (1,2)\n",
    "n_components\n",
    "# Pipeline Vectorizer + LDA\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(max_df = max_df,\n",
    "                    ngram_range = ngram_range),\n",
    "    LatentDirichletAllocation(n_components = 7)\n",
    ")\n",
    "\n",
    "# Fit the pipeline on the cleaned texts\n",
    "pipeline.fit(df[\"goalCleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb4cc3-994d-4873-978e-fc5ee90aa0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the original cleaned texts with the pipeline\n",
    "# Indeed, there is no need to get the vectorized texts first since it's done through the Pipeline\n",
    "\n",
    "# Document Mixture with the Pipeline:\n",
    "document_mixture = pipeline.transform(df[\"goalCleaned\"]) \n",
    "document_mixture.shape \n",
    "\n",
    "# Topic Mixture with the Pipeline:\n",
    "topic_mixture = pd.DataFrame(pipeline._final_estimator.components_,columns = pipeline[0].get_feature_names_out())\n",
    "topic_mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b92ac8-9d23-4d05-b290-af4cbfcefa37",
   "metadata": {},
   "source": [
    "# Specialities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e7cc8-632c-41c6-b35f-22080cab3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_cols=['specialities', 'population', 'focus','metaGoalTitle']\n",
    "df_txt=df.copy().loc[:,txt_cols]\n",
    "df_txt=df_txt.select_dtypes(include = ['object'])\n",
    "df_txt.fillna('Unknown',inplace=True)\n",
    "\n",
    "df_txt.specialities=df_txt.specialities.apply(lambda x: x.replace(\" \",\"\").replace(\";\",\",\").replace(\"/\",\",\").split(\",\"))\n",
    "\n",
    "\n",
    "# get a list of all possible specialization\n",
    "spec_list=[]\n",
    "for row in df_txt.specialities:\n",
    "    spec_list+=row\n",
    "\n",
    "words=list(set(spec_list))\n",
    "counts=[spec_list.count(word) for word in words]\n",
    "\n",
    "spec_list_df=pd.DataFrame({\n",
    "    'specialities': words,\n",
    "    'count':counts\n",
    "\n",
    "})\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd2848-81e1-412c-8e75-7070e509e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.specialities.str.split(',')\n",
    "df.focus.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673a556-c8a3-4c1e-ab23-2c9401e64c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.pos_tag(['feet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e86f9-4c74-436f-a921-ed03ec9ef60a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
